{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyDOE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import namedtuple\n",
    "from matplotlib import cm\n",
    "import cma\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "import sys\n",
    "import bbobbenchmarks as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Functions for Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' F2 '''\n",
    "def F2(X):\n",
    "    f = bn.F2()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F3 '''\n",
    "def F3(X):\n",
    "    f = bn.F3()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F7 '''\n",
    "def F7(X):\n",
    "    f = bn.F7()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F9 '''\n",
    "def F9(X):\n",
    "    f = bn.F9()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F10 '''\n",
    "def F10(X):\n",
    "    f = bn.F10()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F13 '''\n",
    "def F13(X):\n",
    "    f = bn.F13()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F15 '''\n",
    "def F15(X):\n",
    "    f = bn.F15()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F16 '''\n",
    "def F16(X):\n",
    "    f = bn.F16()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F20 '''\n",
    "def F20(X):\n",
    "    f = bn.F20()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F24 '''\n",
    "def F24(X):\n",
    "    f = bn.F24()\n",
    "    X = np.array(X)\n",
    "    return f(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueRange = namedtuple('ValueRange', ['min', 'max'])\n",
    "\n",
    "def determinerange(values):\n",
    "    \"\"\"Determine the range of values in each dimension\"\"\"\n",
    "    return ValueRange(np.min(values, axis=0), np.max(values, axis=0))\n",
    "\n",
    "\n",
    "def linearscaletransform(values, *, range_in=None, range_out=ValueRange(0, 1), scale_only=False):\n",
    "    \"\"\"Perform a scale transformation of `values`: [range_in] --> [range_out]\"\"\"\n",
    "\n",
    "    if range_in is None:\n",
    "        range_in = determinerange(values)\n",
    "    elif not isinstance(range_in, ValueRange):\n",
    "        range_in = ValueRange(*range_in)\n",
    "\n",
    "    if not isinstance(range_out, ValueRange):\n",
    "        range_out = ValueRange(*range_out)\n",
    "\n",
    "    scale_out = range_out.max - range_out.min\n",
    "    scale_in = range_in.max - range_in.min\n",
    "\n",
    "    if scale_only:\n",
    "        scaled_values = (values / scale_in) * scale_out\n",
    "    else:\n",
    "        scaled_values = (values - range_in.min) / scale_in\n",
    "        scaled_values = (scaled_values * scale_out) + range_out.min\n",
    "\n",
    "    return scaled_values\n",
    "\n",
    "''' Latin HyperCube Sampling Design of Experiment '''\n",
    "def DOE(n_obs, dim):\n",
    "    np.random.seed(0)\n",
    "    lhd = pyDOE.lhs(n=dim, samples=n_obs, criterion='m')\n",
    "    X = [lhd[:,idx] for idx in range(dim)]\n",
    "    return X\n",
    "\n",
    "''' Generate Training Data using LHD along side the Output'''\n",
    "def generate_training_data(n_obs, dim):\n",
    "    X = DOE(n_obs, dim)\n",
    "    X  = [ linearscaletransform(X[idx] , range_out=(-5,5)) for idx in range(dim) ]\n",
    "    X = [ X[idx].reshape(n_obs,1) for idx in range(len(X)) ]\n",
    "    f2_evaluation , f3_evaluation ,  f7_evaluation ,f9_evaluation , f10_evaluation , f13_evaluation , f15_evaluation , f16_evaluation , f20_evaluation , f24_evaluation= np.zeros(X[0].shape[0]) , np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]) , np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0])\n",
    "    for i in range(X[1].shape[0]):\n",
    "        f2_evaluation [i], f3_evaluation [i], f7_evaluation [i],f9_evaluation [i], f10_evaluation [i], f13_evaluation [i], f15_evaluation [i], f16_evaluation [i], f20_evaluation [i] , f24_evaluation [i]= F2(np.concatenate(X, 1)[i]) , F3(np.concatenate(X, 1)[i]), F7(np.concatenate(X, 1)[i]), F9(np.concatenate(X, 1)[i]), F10(np.concatenate(X, 1)[i]), F13(np.concatenate(X, 1)[i]), F15(np.concatenate(X, 1)[i]), F16(np.concatenate(X, 1)[i]), F20(np.concatenate(X, 1)[i]), F24(np.concatenate(X, 1)[i])\n",
    "    train_2 , train_3 , train_7, train_9, train_10, train_13 , train_15, train_16, train_20, train_24 = pd.DataFrame() , pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    for i in range(len(X)):\n",
    "        train_2 [str('X'+str(i+1))] , train_3 [str('X'+str(i+1))], train_7 [str('X'+str(i+1))], train_9 [str('X'+str(i+1))], train_10 [str('X'+str(i+1))], train_13 [str('X'+str(i+1))], train_15 [str('X'+str(i+1))], train_16 [str('X'+str(i+1))], train_20 [str('X'+str(i+1))], train_24 [str('X'+str(i+1))] = X[i].reshape(n_obs,) , X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,)\n",
    "    train_2 ['Y'] , train_3 ['Y'], train_7 ['Y'], train_9 ['Y'], train_10 ['Y'], train_13 ['Y'], train_15 ['Y'], train_16 ['Y'], train_20 ['Y'], train_24 ['Y'] = pd.Series(f2_evaluation) , pd.Series(f3_evaluation), pd.Series(f7_evaluation), pd.Series(f9_evaluation), pd.Series(f10_evaluation), pd.Series(f13_evaluation), pd.Series(f15_evaluation), pd.Series(f16_evaluation), pd.Series(f20_evaluation), pd.Series(f24_evaluation)\n",
    "    train_2.to_csv('Training_Data_Sets\\\\train_2_'+str(n_obs)+'Samples.csv') , train_3.to_csv('Training_Data_Sets\\\\train_3_'+str(n_obs)+'Samples.csv'), train_7.to_csv('Training_Data_Sets\\\\train_7_'+str(n_obs)+'Samples.csv'), train_9.to_csv('Training_Data_Sets\\\\train_9_'+str(n_obs)+'Samples.csv'), train_10.to_csv('Training_Data_Sets\\\\train_10_'+str(n_obs)+'Samples.csv'), train_13.to_csv('Training_Data_Sets\\\\train_13_'+str(n_obs)+'Samples.csv'), train_15.to_csv('Training_Data_Sets\\\\train_15_'+str(n_obs)+'Samples.csv'), train_16.to_csv('Training_Data_Sets\\\\train_16_'+str(n_obs)+'Samples.csv'), train_20.to_csv('Training_Data_Sets\\\\train_20'+str(n_obs)+'Samples.csv'), train_24.to_csv('Training_Data_Sets\\\\train_24'+str(n_obs)+'Samples.csv')\n",
    "    return train_2 ,train_3, train_7, train_9, train_10, train_13, train_15, train_16, train_20, train_24\n",
    "\n",
    "''' Generate Test Data using LHD along side the Output'''\n",
    "def generate_test_data(n_obs,dim):\n",
    "    X = DOE(n_obs, dim)\n",
    "    X  = [ linearscaletransform(X[idx] , range_out=(-5,5)) for idx in range(dim) ]\n",
    "    X = [ X[idx].reshape(n_obs,1) for idx in range(len(X)) ]\n",
    "    f2_evaluation , f3_evaluation ,  f7_evaluation ,f9_evaluation , f10_evaluation , f13_evaluation , f15_evaluation , f16_evaluation , f20_evaluation , f24_evaluation= np.zeros(X[0].shape[0]) , np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]) , np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0]), np.zeros(X[0].shape[0])\n",
    "    for i in range(X[1].shape[0]):\n",
    "        f2_evaluation [i] , f3_evaluation [i], f7_evaluation [i],f9_evaluation [i], f10_evaluation [i], f13_evaluation [i], f15_evaluation [i] , f16_evaluation [i], f20_evaluation [i], f24_evaluation [i]= F2(np.concatenate(X, 1)[i]) , F3(np.concatenate(X, 1)[i]), F7(np.concatenate(X, 1)[i]), F9(np.concatenate(X, 1)[i]), F10(np.concatenate(X, 1)[i]), F13(np.concatenate(X, 1)[i]), F15(np.concatenate(X, 1)[i]), F16(np.concatenate(X, 1)[i]), F20(np.concatenate(X, 1)[i]), F24(np.concatenate(X, 1)[i])\n",
    "    test_2 , test_3 , test_7, test_9, test_10, test_13 , test_15, test_16, test_20, test_24 = pd.DataFrame() , pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    for i in range(len(X)):\n",
    "        test_2 [str('X'+str(i+1))] , test_3 [str('X'+str(i+1))], test_7 [str('X'+str(i+1))], test_9 [str('X'+str(i+1))], test_10 [str('X'+str(i+1))], test_13 [str('X'+str(i+1))], test_15 [str('X'+str(i+1))], test_16 [str('X'+str(i+1))], test_20 [str('X'+str(i+1))], test_24 [str('X'+str(i+1))] = X[i].reshape(n_obs,) , X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,), X[i].reshape(n_obs,)\n",
    "    test_2 ['Y'] , test_3 ['Y'], test_7 ['Y'], test_9 ['Y'], test_10 ['Y'], test_13 ['Y'], test_15 ['Y'], test_16 ['Y'], test_20 ['Y'], test_24 ['Y'] = pd.Series(f2_evaluation) , pd.Series(f3_evaluation), pd.Series(f7_evaluation), pd.Series(f9_evaluation), pd.Series(f10_evaluation), pd.Series(f13_evaluation), pd.Series(f15_evaluation), pd.Series(f16_evaluation), pd.Series(f20_evaluation), pd.Series(f24_evaluation)\n",
    "    test_2.to_csv('Test_Data_Sets\\\\test_2_'+str(n_obs)+'Samples.csv') , test_3.to_csv('Test_Data_Sets\\\\test_3_'+str(n_obs)+'Samples.csv'), test_7.to_csv('Test_Data_Sets\\\\test_7_'+str(n_obs)+'Samples.csv'), test_9.to_csv('Test_Data_Sets\\\\test_9_'+str(n_obs)+'Samples.csv'), test_10.to_csv('Test_Data_Sets\\\\test_10_'+str(n_obs)+'Samples.csv'), test_13.to_csv('Test_Data_Sets\\\\test_13_'+str(n_obs)+'Samples.csv'), test_15.to_csv('Test_Data_Sets\\\\test_15_'+str(n_obs)+'Samples.csv'), test_16.to_csv('Test_Data_Sets\\\\test_16_'+str(n_obs)+'Samples.csv'), test_20.to_csv('Test_Data_Sets\\\\test_20_'+str(n_obs)+'Samples.csv'), test_24.to_csv('Test_Data_Sets\\\\test_24_'+str(n_obs)+'Samples.csv')\n",
    "    return test_2 ,test_3, test_7, test_9, test_10, test_13, test_15, test_16, test_20, test_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n",
      "No. of Duplicate rows between Train & Test Data Set are: 0\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "multiple = [20]\n",
    "for i in range(len(multiple)):\n",
    "    train_2,train_3,train_7,train_9,train_10,train_13,train_15,train_16,train_20,train_24 = generate_training_data(dim * multiple[i], dim)\n",
    "test_2,test_3,test_7,test_9,test_10,test_13,test_15,test_16,test_20,test_24 = generate_test_data(int(dim * 20 * 0.2), dim)\n",
    "data_2 , data_3, data_7, data_9, data_10, data_13, data_15, data_16, data_20, data_24 = pd.concat([train_2, test_2]) , pd.concat([train_3, test_3]), pd.concat([train_7, test_7]), pd.concat([train_9, test_9]), pd.concat([train_10, test_10]), pd.concat([train_13, test_13]), pd.concat([train_15, test_15]), pd.concat([train_16, test_16]), pd.concat([train_20, test_20]), pd.concat([train_24, test_24])\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_2.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_3.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_7.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_9.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_10.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_13.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_15.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_16.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_20.duplicated().sum()))\n",
    "print ('No. of Duplicate rows between Train & Test Data Set are: '+str(data_24.duplicated().sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
